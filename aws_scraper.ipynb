{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "206629e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e054ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_YEAR = datetime.today().strftime('20%y')\n",
    "\n",
    "BASE_URL = \"https://service.stuttgart.de/lhs-services/aws/abfuhrtermine\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0\"\n",
    "}\n",
    "\n",
    "# Change to your street and house number\n",
    "ADDRESS = {\n",
    "    \"calendar[street]\": \"Neue Str.\",\n",
    "    \"calendar[streetnr]\": \"9A\",\n",
    "    \"calendar[datefrom]\": f\"01.01.{CURRENT_YEAR}\",\n",
    "    \"calendar[dateto]\": f\"31.01.{int(CURRENT_YEAR)+1}\",\n",
    "    \"calendar[wastetype][]\": [\"restmuell\", \"biomuell\", \"altpapier\", \"gelbersack\"],\n",
    "    \"calendar[submit]\": \"Abfuhrtermine ermitteln\"\n",
    "}\n",
    "\n",
    "FILENAME = f\"{CURRENT_YEAR}_abfuhrtermine.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07af541e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_abfuhrtermine():\n",
    "    session = requests.Session()\n",
    "    response = session.post(BASE_URL, headers=HEADERS, data=ADDRESS)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    results = {}\n",
    "    current_type = None\n",
    "\n",
    "    rows = soup.select(\"table#awstable tr\")\n",
    "    for row in rows:\n",
    "        header = row.find(\"th\")\n",
    "        if header:\n",
    "            # Extract internal value like \"restmuell\", \"biomuell\" from text\n",
    "            text = header.text.strip().lower()\n",
    "            if \"restabfall\" in text:\n",
    "                current_type = \"restmuell\"\n",
    "            elif \"bioabfall\" in text:\n",
    "                current_type = \"biomuell\"\n",
    "            elif \"altpapier\" in text:\n",
    "                current_type = \"altpapier\"\n",
    "            elif \"gelber sack\" in text:\n",
    "                current_type = \"gelbersack\"\n",
    "            else:\n",
    "                current_type = text  # fallback\n",
    "            results[current_type] = []\n",
    "        else:\n",
    "            cols = row.find_all(\"td\")\n",
    "            if len(cols) >= 2 and current_type:\n",
    "                date = cols[1].text.strip()\n",
    "                results[current_type].append(date)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0ab6d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_json(data):\n",
    "    with open(FILENAME, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"Saved {len(data)} entries to {FILENAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12c622cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_and_save_abfuhrtermine():\n",
    "    if not os.path.exists(FILENAME):\n",
    "        deprecated_file = f\"{int(CURRENT_YEAR)-1}_abfuhrtermine.json\"\n",
    "        if os.path.exists(deprecated_file):\n",
    "            os.remove(deprecated_file)\n",
    "            print(f\"Deleted deprecated file: {deprecated_file}\")\n",
    "        \n",
    "    new_data = scrape_abfuhrtermine()\n",
    "    save_json(new_data)\n",
    "    print(f\"Created new file {FILENAME} with {len(new_data)} entries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1ccac50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 4 entries to 2025_abfuhrtermine.json\n",
      "Created new file 2025_abfuhrtermine.json with 4 entries.\n"
     ]
    }
   ],
   "source": [
    "update_and_save_abfuhrtermine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8f46d1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_tomorrow_matches():\n",
    "    tomorrow = (datetime.today() + timedelta(days=1)).strftime(\"%d.%m.%Y\")\n",
    "\n",
    "    with open(FILENAME) as f: \n",
    "        data = json.load(f)\n",
    "        matched_types = []\n",
    "        for type, dates in data.items():\n",
    "            if tomorrow in dates:\n",
    "                matched_types.append(type)\n",
    "                continue\n",
    "    return matched_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aff2e5ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['biomuell']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_tomorrow_matches()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws_stuttgart",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
